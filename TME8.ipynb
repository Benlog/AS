{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gr[theta, gamma] = [ gr[theta] log P(h/x) ] Delta ( f[theta](x, h) ) + gr[gamma] (f[theta] (x, h), y) )\n",
    "\n",
    "-> gr[pi] = gr log P( a / s[y] ) P ( ... )\n",
    "          = [ Somme[T] gr log P (a[t] / s[t] ) P ( ... )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for t in range(0, T):\n",
    "    log P = model(s)\n",
    "    a = log.   ().multinominal()\n",
    "    logPa = log P[a]\n",
    "    l += logPa\n",
    "    s, reward, done, ... = env.step(a)\n",
    "    r+= reward\n",
    "-> r = Somme R(ai / si)\n",
    "-> l = Somme log P(ai / si)\n",
    "(-l*r=.backward()\n",
    "(-l*(r-moy(r))).backward()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l = log(Pj(H/x))\n",
    "h = l.multinomial()\n",
    "out = fo(x,h)\n",
    "loss = (out - y) * (out - y)\n",
    "loss.mean().backward()\n",
    "h.reinforce(-loss)\n",
    "#l.backward(None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logP = g(x)\n",
    "h = g(x).exp().multinomial()\n",
    "h2 = Variable(h.data)\n",
    "out = f(x,h2)\n",
    "loss = (out - y) * (out - y)\n",
    "loss2 = Variable(loss.data)\n",
    "loss.mean().backward()\n",
    "(logP * loss.data).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choose(object):\n",
    "    def __init__(self, chooser, netList):\n",
    "        '''\n",
    "        :param chooser: network N*X -> N*T\n",
    "        :param embedDict: list of T > network'''\n",
    "        self.chooser = chooser\n",
    "        self.netList = netList\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logP = self.chooser(x)\n",
    "        h = logP.exp().multinomial()\n",
    "        out = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.module):\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.module):\n",
    "    def forward(self, x, h):\n",
    "        return h*self.linear1(x) + (1-h) * self.linear2(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comment d√©bugguer\n",
    "2 optimizer (pour F et G)\n",
    "tester avec pasGrad pour G (pG(G))= 0\n",
    "pG(G) << pG(F)\n",
    "p(0)*log(p(0)) + p(1) * log(p(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
